{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "756aad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def data_to_dict(name):\n",
    "    \"\"\"\n",
    "   Принимает имя датасета в котором есть колонки \"question\",\"final_decision\"\n",
    "\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(name)\n",
    "    data = data[[\"question\",\"final_decision\"]]\n",
    "    data = data.rename(columns = {\"final_decision\":\"answer\"})\n",
    "    return data.to_dict(orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e275f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_to_dict(\"PubMedQATrain.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3de46225",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel:\n",
    "    def predict(self, question: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Возвращает один из: 'yes', 'no', 'maybe'\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "class GPTPromptModel(BaseModel):\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def predict(self, question):\n",
    "        prompt = f\"Question: {question} Answer (yes/no/maybe):\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        output = self.model.generate(**inputs, max_new_tokens=2)\n",
    "        full_output = self.tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "        if full_output.lower().startswith(prompt.lower()):\n",
    "            answer = full_output[len(prompt):].strip()\n",
    "        else:\n",
    "            answer = full_output.strip()\n",
    "        if len(answer)>1 and answer[-1] == '.':\n",
    "            answer = answer[:-1]\n",
    "        return answer.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6537a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "def get_lora_model(path):\n",
    "    peft_config = PeftConfig.from_pretrained(path)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path)\n",
    "    model = PeftModel.from_pretrained(base_model, path)\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4577e3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=2304, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): Conv1D(nf=768, nx=768)\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D(nf=3072, nx=768)\n",
       "              (c_proj): Conv1D(nf=768, nx=3072)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_ft_low = get_lora_model(\"D:\\Machine Learning\\Labs and practice\\Keggle\\PortFolio\\PubMedGPT\\gpt2-medical-lora\\checkpoint-624\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681e5753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "Reports = []\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    \"openai-community/gpt2-xl\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model_ft_low_init = GPTPromptModel(model_ft_low, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cc8f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Models = []\n",
    "Models.append(GPTPromptModel(model_base, tokenizer))\n",
    "Models.append(GPTPromptModel(model_ft_low_init, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991879e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt2': {'yes': {'precision': 0.6,\n",
       "   'recall': 0.02717391304347826,\n",
       "   'f1-score': 0.05199306759098787,\n",
       "   'support': 552.0},\n",
       "  'no': {'precision': 0.4606741573033708,\n",
       "   'recall': 0.12130177514792899,\n",
       "   'f1-score': 0.1920374707259953,\n",
       "   'support': 338.0},\n",
       "  'maybe': {'precision': 0.0,\n",
       "   'recall': 0.0,\n",
       "   'f1-score': 0.0,\n",
       "   'support': 110.0},\n",
       "  'micro avg': {'precision': 0.49122807017543857,\n",
       "   'recall': 0.056,\n",
       "   'f1-score': 0.10053859964093358,\n",
       "   'support': 1000.0},\n",
       "  'macro avg': {'precision': 0.353558052434457,\n",
       "   'recall': 0.049491896063802415,\n",
       "   'f1-score': 0.08134351277232772,\n",
       "   'support': 1000.0},\n",
       "  'weighted avg': {'precision': 0.48690786516853934,\n",
       "   'recall': 0.056,\n",
       "   'f1-score': 0.09360883841561173,\n",
       "   'support': 1000.0}}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluator import Evaluator\n",
    "i = 0\n",
    "for model in Models:\n",
    "    evaluator = Evaluator(model_int)\n",
    "    report = evaluator.evaluate(data)\n",
    "    Reports.append({\"gpt2-\" + str(i):report})\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
